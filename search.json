[
  {
    "objectID": "modify.html",
    "href": "modify.html",
    "title": "modify",
    "section": "",
    "text": "source\n\nadd_poseflow_figures\n\n add_poseflow_figures (input_detections, json_path)\n\n\nsource\n\n\noutput_alphapose_json\n\n output_alphapose_json (poses_series, figure_type='figures')\n\n\nsource\n\n\ninterpolate_missing_coords\n\n interpolate_missing_coords (input_frames, threshold=0.5,\n                             flip_figures=False, check_bbox=False,\n                             all_visible=True, overlap_threshold=0.7,\n                             video_file=None, figure_type='figures')\n\n\nsource\n\n\ntrim_empty_frames_start_end\n\n trim_empty_frames_start_end (input_frames, figure_type='figures')\n\n\nsource\n\n\ninterpolate_missing_poses\n\n interpolate_missing_poses (input_frames, threshold=0.2, video_file=None,\n                            figure_type='figures', trim_ends=True)\n\n\nsource\n\n\nis_usable_pose\n\n is_usable_pose (frame_info, threshold=0.2, figure_type='figures')\n\n\nsource\n\n\nadd_flipped_zeroified_figures\n\n add_flipped_zeroified_figures (input_frames, add_flipped=True,\n                                add_zerofied=True, figure_type='figures')\n\n\nsource\n\n\ncorrect_pose\n\n correct_pose (input_coords)\n\n\nsource\n\n\nright_ankle_from_knee\n\n right_ankle_from_knee (coords, missing_coords)\n\n\nsource\n\n\nleft_ankle_from_knee\n\n left_ankle_from_knee (coords, missing_coords)\n\n\nsource\n\n\nright_hip_btwn_shoulder_knee_ankle\n\n right_hip_btwn_shoulder_knee_ankle (coords, missing_coords)\n\n\nsource\n\n\nleft_hip_btwn_shoulder_knee_ankle\n\n left_hip_btwn_shoulder_knee_ankle (coords, missing_coords)\n\n\nsource\n\n\nright_elbow_btwn_shoulder_wrist\n\n right_elbow_btwn_shoulder_wrist (coords, missing_coords)\n\n\nsource\n\n\nleft_elbow_btwn_shoulder_wrist\n\n left_elbow_btwn_shoulder_wrist (coords, missing_coords)\n\n\nsource\n\n\nright_ear_btwn_eye_shoulder\n\n right_ear_btwn_eye_shoulder (coords, missing_coords)\n\n\nsource\n\n\nleft_ear_btwn_eye_shoulder\n\n left_ear_btwn_eye_shoulder (coords, missing_coords)\n\n\nsource\n\n\nright_eye_btwn_nose_shoulder\n\n right_eye_btwn_nose_shoulder (coords, missing_coords)\n\n\nsource\n\n\nleft_eye_btwn_nose_shoulder\n\n left_eye_btwn_nose_shoulder (coords, missing_coords)\n\n\nsource\n\n\nnose_btwn_eyes_ears_shoulders\n\n nose_btwn_eyes_ears_shoulders (coords, missing_coords)\n\n\nsource\n\n\ncount_figures_and_time\n\n count_figures_and_time (input_frames, figure_type='figures')\n\n\nsource\n\n\naverage_coords\n\n average_coords (coord1, coord2)\n\n\nsource\n\n\nshift_figure\n\n shift_figure (coords_and_confidence, dx, dy)\n\n\nsource\n\n\nget_union\n\n get_union (a, b)\n\n\nsource\n\n\nget_intersect\n\n get_intersect (a, b)\n\n\nsource\n\n\nin_bbox_check\n\n in_bbox_check (coord, bbox, margin=0.5)\n\n\nsource\n\n\nget_bbox_area\n\n get_bbox_area (bbox)\n\n\nsource\n\n\nget_bbox\n\n get_bbox (pose_coords, move_to_origin=False, margin=0, width=None,\n           height=None)\n\n\nsource\n\n\nzeroify_detections_y_first\n\n zeroify_detections_y_first (input_detections, width=None, height=None)\n\nVersion of zeroify_detections from above that assumes Y, X order of coordinates, again with Y=0 at top left\n\nsource\n\n\nzeroify_detections\n\n zeroify_detections (input_detections, width=None, height=None)\n\nModifies a figure’s coordinates so that the corner of its bounding box is at 0,0. This is mostly for visualization with PIL images, (note that PIL puts y=0 at the top). The modifications are done for all figures in a single frame.\n\nsource\n\n\nflip_detections_y_first\n\n flip_detections_y_first (input_detections, flip_y=False, flip_x=False,\n                          rectify_x=False,\n                          mirror_coco_17_left_right=False)\n\nMirror the coordinates of a pose in place around the midpoint of either the Y or X axis. The former is sometimes necessary when working with image coordinates (0,0 at top left) vs. figure coordiantes (0,0 at bottom left). The latter is useful when the pose needs to be flipped as though it is being viewed in a mirror (which is often the case with dance videos).\nThis function flips all detected poses in a single frame. If the rectify_x flag is set, this function can also count how many coords are on the left or right side of the pose, and mirror the coordinates horizontally so that the most coords are always on stage right (viewer’s left) – in which case the parameter value of flip_x is ignored.\n\nsource\n\n\nflip_detections\n\n flip_detections (input_detections, flip_y=False, flip_x=False,\n                  rectify_x=False, y_first=False,\n                  mirror_coco_17_left_right=False)\n\nMirror the coordinates of a pose in place around the midpoint of either the Y or X axis. The former is sometimes necessary when working with image coordinates (0,0 at top left) vs. figure coordiantes (0,0 at bottom left). The latter is useful when the pose needs to be flipped as though it is being viewed in a mirror (which is often the case with dance videos).\nThis function flips all detected poses in a single frame. If the rectify_x flag is set, this function can also count how many coords are on the left or right side of the pose, and mirror the coordinates horizontally so that the most coords are always on stage right (viewer’s left) – in which case the parameter value of flip_x is ignored.\n\nsource\n\n\nget_figure_coords_y_first\n\n get_figure_coords_y_first (coords_and_confidence, margin=0)\n\n\nsource\n\n\nget_figure_coords\n\n get_figure_coords (coords_and_confidence, margin=0)"
  },
  {
    "objectID": "pifpafpose_detector.html",
    "href": "pifpafpose_detector.html",
    "title": "pifpafpose_detector",
    "section": "",
    "text": "source\n\nDetector\n\n Detector ()\n\nGiven a still image (or video frame), finds poses.\nAttributes:\ndevice: PyTorch computing resource (GPU or CPU)\nnet: Pose detection neural network model\nprocessor: Pose detection image processor\n\nteddy = Detector()\n#detections = teddy.detect_image('sample_data/sample1.png', viz=True)\n#pose_output = teddy.detect_video(\"/srv/choreo/Einstein.mp4\", start_seconds=6310, end_seconds=7288, write_images=True, images_too=False)\n\n\nteddy.init_model()\n\n\nFPS = 25\n!ffmpeg -y -framerate $FPS -pattern_type glob -i 'video_folder/*.png' -strict '-2' -c:v libx264 -vf \"fps=$FPS\" -pix_fmt yuv420p Einstein_ballet_pf.mp4\n\n\ntry:\n    teddy = Detector()\n    detections = teddy.detect_image('sample_data/sample1.png')\n    print(detections[0])\nexcept:\n    print(\"Unable to instantiate a detector on your system. Do you have PyTorch with CUDA enabled?\")"
  },
  {
    "objectID": "movenet_detector.html",
    "href": "movenet_detector.html",
    "title": "movenet_detector",
    "section": "",
    "text": "source\n\nDetector\n\n Detector ()\n\nGiven a still image (or video frame), finds poses.\nAttributes:\nmodule: The pose detection module being used for inference input_size: The input resolution of the model model_names: Shorthand dict of pose detection models that should be available locally, with their local folder name and download URLs as values\n\nsource\n\n\nDetection\n\n Detection (data)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\ndisplay_img_array\n\n display_img_array (ima)\n\n\n#teddy = Detector()\n#teddy.init_model()"
  },
  {
    "objectID": "matrixify.html",
    "href": "matrixify.html",
    "title": "matrixify",
    "section": "",
    "text": "source\n\ncompare_laplacians\n\n compare_laplacians (p1, p2, figure_index=0,\n                     figure_type='flipped_figures', show=False)\n\n\nsource\n\n\nget_laplacian_matrix\n\n get_laplacian_matrix (frame, normalized=True, show=False, figure_index=0,\n                       figure_type='flipped_figures')\n\nLAPLACIAN: compute the Delaunay triangulation between keypoints, then use the connections to build an adjacency matrix, which is then converted to its (normalized) Laplacian matrix (a single matrix that encapsulates the degree of each node and the connections between the nodes). Then you can subtract a pose’s Laplacian from another’s to get a measure of the degree of similarity or difference between them.\n\nsource\n\n\nget_pose_matrix\n\n get_pose_matrix (frame, figure_index=0, figure_type='flipped_figures')\n\n\nsource\n\n\ncompare_poses_cosine\n\n compare_poses_cosine (p1, p2)\n\n\nsource\n\n\nnormalize_and_compare_poses_cosine\n\n normalize_and_compare_poses_cosine (p1, p2)\n\n\n\n\n\nDetails\n\n\n\n\np1\n\n\n\np2\nUses cosine distance\n\n\n\n\nsource\n\n\nnormalize_symmetrify_and_compare_poses_cosine\n\n normalize_symmetrify_and_compare_poses_cosine (p1, p2)\n\n\nsource\n\n\nsymmetrify_pose\n\n symmetrify_pose (frame, figure_index=0, figure_type='figures',\n                  y_first=True)\n\n\nsource\n\n\nnormalize_pose\n\n normalize_pose (frame, figure_index=0, figure_type='figures', norm='l2',\n                 y_first=True, flip_x=False, flip_y=False,\n                 mirror_coco_17_left_right=False)\n\n\nsource\n\n\nget_normalized_coords\n\n get_normalized_coords (frame, figure_index=0, figure_type='figures',\n                        norm='l2')\n\n\nsource\n\n\nmatrixify_pose\n\n matrixify_pose (coords_and_confidence)\n\nDISTANCE MATRIX: compute a pose’s L1-normed inter-keypoint distance matrix. To compare any two poses, we can measure the degree of correlation between their distance matrices via a statistical test, such as the Mantel test. XXX It’s not obvious that normalizing the matrix really makes a difference to the final correlation comparison, but it doesn’t seem to hurt, either… Note that if the pose representation has 17 keypoints, then each pose instance can be represented by a condensed distance matrix (or vector) of 136 elements."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "choreo_k",
    "section": "",
    "text": "Excerpts from analyses of BTS’s Fire and Jennie’s SOLO"
  },
  {
    "objectID": "index.html#cite",
    "href": "index.html#cite",
    "title": "choreo_k",
    "section": "Cite",
    "text": "Cite\nThis repository contains code and data used in the paper Comparative K-Pop Choreography Analysis through Deep-Learning Pose Estimation across A Large Video Corpus:\n@article{btdhq2021151,\n    author = {Peter Broadwell and Timothy R. Tangherlini},\n    title = {Comparative K-Pop Choreography Analysis through Deep-Learning Pose Estimation across A Large Video Corpus},\n    journal = {Digital Humanities Quarterly},\n    volume = {15},\n    number = {1},\n    year = 2021\n}"
  },
  {
    "objectID": "index.html#try",
    "href": "index.html#try",
    "title": "choreo_k",
    "section": "Try",
    "text": "Try\nThe full Python code package documented here is still under development. To try out the pose analysis code in the meantime, we recommend opening the notebook Pose_analysis_examples.ipynb in Google Colab:"
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "choreo_k",
    "section": "Documentation",
    "text": "Documentation\nDetails of the specific functions of each module are available via the project documentation site.\nGenerally, though, choreo_k is intended to provide the pose analysis pipeline routine illustrated in the following figure, in which each step of the routine can be implemented in one or more different ways, but each implementation provides roughly equivalent functionality. For example, different third-party pose detection libraries may be used to generate pose data, which then can be modified, represented, analyzed and visualized by any modules that support a specific library’s pose data output format.\n\n\n\nAn illustration of the steps of the archetypal pose analysis routine"
  },
  {
    "objectID": "visualize.html",
    "href": "visualize.html",
    "title": "visualize",
    "section": "",
    "text": "source\n\nviz_dist_matrices\n\n viz_dist_matrices (p1, p2, figure_type='flipped_figures')\n\n\nsource\n\n\ndraw_figure\n\n draw_figure (point_weights=None, show=True)\n\nScale keypoint radii by how much they moved in a video\n\nsource\n\n\noverlay_video\n\n overlay_video (video_file, pose_data, plot_type='pose',\n                source_figure='figures', show_axis=False, savedir='',\n                start_frame=0)\n\nSet savedir to a folder where a whole bunch of images from the video, with pose overlays drawn on them, will be stored. These can be turned into a video (poses_video.mp4 or similar) later using this command: !ffmpeg -y -framerate \\(FPS -pattern_type glob -i 'savedir/*.png' -strict '-2' -c:v libx264 -vf \"fps=\\)FPS” -pix_fmt yuv420p poses_video.mp4 Note that both occurrences of $FPS should be replaced with the framerate of the video, which can be obtained from get_video_stats(video_filename)\n\nsource\n\n\noverlay_poses\n\n overlay_poses (pil_image, figures_frame, show=False, plot_type='pose',\n                source_figure='figures', show_axis=False, savepath='')\n\n\nsource\n\n\nexcerpt_pose\n\n excerpt_pose (video_file, frame_poses, figure_index=0, show=False,\n               plot_type='pose', source_figure='figures',\n               flip_figures=False, margin=0.2, width=None, height=None,\n               show_axis=True)\n\n\nsource\n\n\nfig2img\n\n fig2img (fig2, w=8, h=8, dpi=72)\n\n\nsource\n\n\nplot_delaunay\n\n plot_delaunay (figure, image=None, show=True, show_axis=True)\n\n\nsource\n\n\nplot_poses\n\n plot_poses (detections, image=None, show=True, savepath='',\n             show_axis=True)"
  },
  {
    "objectID": "analyze.html",
    "href": "analyze.html",
    "title": "analyze",
    "section": "",
    "text": "source\n\ncompare_sequences_pairwise\n\n compare_sequences_pairwise (seq1, seq2, figure_type='figures')\n\n\nsource\n\n\nmember_frame_movements\n\n member_frame_movements (movement_series, poses_series, max_clip=3,\n                         show=False, condense=True)\n\nFor a multi-dancer video, generate individual inter-frame movement series for all of the dancers and plot them.\n\nsource\n\n\naverage_frame_movements\n\n average_frame_movements (movement_series, poses_series, show=False,\n                          max_clip=3, video_file=None)\n\nCompute the average and stdv of the inter-frame movement for each frame of a pose sequence with >= 1 dancers. Assumes movement_series is an array of inter-frame movement values, one for each detected pose, with missing poses identified via np.nan, as generated from process_movement_series.\n\nsource\n\n\nplot_interpose_similarity\n\n plot_interpose_similarity (pose_series, frame_means, frame_stdevs,\n                            video_file, show=False, min_clip=0.2)\n\nFor multi-pose videos\n\nsource\n\n\ncompare_multiple\n\n compare_multiple (pose_data, method='distance',\n                   figure_type='aligned_figures')\n\nFor multi-dancer videos: Get the mean and standard deviation of inter-pose similarities for each frame\n\nsource\n\n\ncondense_labels\n\n condense_labels (labels, cluster_map)\n\nCan be used to “collapse” clusters of similar poses into meta-clusters\n\nsource\n\n\ncompute_pose_distribution\n\n compute_pose_distribution (poses_series, labels, descriptors,\n                            figure_type='zeroified_figures',\n                            cluster_averages=None)\n\nAssign non-clustered poses to clusters (can take a long time) and generate a clustering timeline heatmap of the pose occurrences.\n\nsource\n\n\nrender_pose_distribution\n\n render_pose_distribution (heatmap, poses_series, labels, descriptors,\n                           closest_matches=None, show=True,\n                           video_file=None, time_index=None,\n                           cell_height=120, xlim=None)\n\nDraw a pose cluster timeline based on a precomputed clustering and assigment of non-clustered poses to clusters via compute_pose_distribution() below. Passing in the heatmap from the previous step saves a lot of time.\n\nsource\n\n\nfind_nearest_pose\n\n find_nearest_pose (pose_matrix, cluster_averages)\n\n\nsource\n\n\nget_cluster_averages_and_indices\n\n get_cluster_averages_and_indices (labels, descriptors, pose_series,\n                                   figure_type='figures', video_file=None,\n                                   flip_figures=False)\n\nAverage the members of each pose cluster to get the representative “average” pose for the cluster.\n\nsource\n\n\ncluster_poses\n\n cluster_poses (poses_series, figure_type='aligned_figures',\n                min_samples=50)\n\n\nsource\n\n\nget_feature_vectors\n\n get_feature_vectors (pose_series, figure_type='aligned_figures',\n                      method='distance')\n\n\nsource\n\n\naverage_poses\n\n average_poses (pose_series, descriptors,\n                source_figures='zeroified_figures', flip=True)\n\n\nsource\n\n\nprocess_movement_series\n\n process_movement_series (pose_data, pose_index=-1,\n                          figure_type='flipped_figures', video_file=None,\n                          method='distance', interpolate=True, viz=True)\n\nSmooth, summarize, visualize movement data for one or more figures across a time series. Also visualize aggregate movement data for each keypoint, if distance matrix method is used.\n\nsource\n\n\nmovements_time_series\n\n movements_time_series (pose_data, pose_index=-1, method='distance',\n                        figure_type='flipped_figures', video_file=None)\n\nCalculate a time series of the differences between each pair of poses in a sequence. This works with a single figure (pose_index=0) or all the figures in the video (pose_index=-1). It can be run on its own, but typically this is a helper function for process_movement_series() (below).\n\nsource\n\n\nfill_nans_scipy1\n\n fill_nans_scipy1 (padata, pkind='linear')\n\nFill in missing values from a time series, after the first non-NAN value and up to the last non-NAN value. Note that scipy.interpolated.interp1d provides a lot more options (splines, quadratic, etc.)\n\nsource\n\n\ncorrelate_time_series\n\n correlate_time_series (pose_data1, pose_data2, method='correlate',\n                        figure_type='figures')\n\n\nsource\n\n\ncorr_time_series_matrix\n\n corr_time_series_matrix (pose_data, method='distance')\n\nGenerate a full time-series pose similarity heatmap for all available poses and frames from the video. This code can use either pose characterization approach; in practice, the distance matrix-based analyses take longer to calculate but are more accurate.\n\nsource\n\n\nsmooth_series\n\n smooth_series (x, window_len=11, window='flat')\n\nSmooth a time series via a sliding window average From https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html"
  }
]